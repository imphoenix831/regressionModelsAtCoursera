---
title: "Quiz2"
author: "Yao-Jen Kuo"
date: "Sunday, October 18, 2015"
output:
  html_document:
    theme: united
    toc: true
    toc_depth: 2
    highlight: tango
    fig_width: 9
    fig_height: 5
    fig_caption: false
---

# Regression Models At Coursera

## Quiz 2

### Question 1

Consider the following data with x as the predictor and y as as the outcome.

```{r}
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
```

Give a P-value for the two sided hypothesis test of whether $beta_1$ from a linear regression model is 0 or not.

```{r}
fit <- lm(y~x)
summary(fit)
```

Answer  | Options
------- | -------
|  | 0.391 
V  | 0.05296
|  | 0.025
|  | 2.325

### Question 2

Consider the previous problem, give the estimate of the residual standard deviation.

```{r}
residual <- resid(fit)
squaredResidual <- residual^2
residualVariance <- sum(squaredResidual) / (length(residual) - 2)
sqrt(residualVariance)
```

Answer  | Options
------- | -------
|  | 0.3552 
|  | 0.4358
|  | 0.05296
V  | 0.223

### Question 3

In the `mtcars` data set, fit a linear regression model of weight (predictor) on mpg (outcome). Get a 95% confidence interval for the expected mpg at the average weight. What is the lower endpoint?

```{r}
data(mtcars)
fit <- lm(mpg ~ wt, mtcars)
exp <- fit$coef[1] + mean(mtcars$wt) * fit$coef[2]
exp - 2 * summary(fit)$coef[,2][2]
```

Answer  | Options
------- | -------
|  | -4.00
|  | -6.486
|  | 21.190
V  | 18.991

### Question 4

Refer to the previous question. Read the help file for `mtcars`. What is the weight coefficient interpreted as?

Answer  | Options
------- | -------
|  | The estimated expected change in mpg per 1 lb increase in weight.
V  | The estimated expected change in mpg per 1,000 lb increase in weight.
|  | The estimated 1,000 lb change in weight per 1 mpg increase.
|  | It can't be interpreted without further information

### Question 5

Consider again the `mtcars` data set and a linear regression model with mpg as predicted by weight (1,000 lbs). A new car is coming weighing 3000 pounds. Construct a 95% prediction interval for its mpg. What is the upper endpoint?

```{r}
fit[[1]][1] + fit[[1]][2]*(3000/1000)
```

Answer  | Options
------- | -------
|  | 14.93
|  | -5.77
V  | 21.25
|  | 27.57

### Question 6

Consider again the `mtcars` data set and a linear regression model with mpg as predicted by weight (in 1,000 lbs). A “short” ton is defined as 2,000 lbs. Construct a 95% confidence interval for the expected change in mpg per 1 short ton increase in weight. Give the lower endpoint.

```{r}
(2000/1000) * (fit$coef[2] - 2 * summary(fit)$coef[,2][2])
```

Answer  | Options
------- | -------
V  | -12.973(?)
|  | 4.2026
|  | -6.486
|  | -9.000

### Question 7

If my X from a linear regression is measured in centimeters and I convert it to meters what would happen to the slope coefficient?

Answer  | Options
------- | -------
|  | It would get multiplied by 10.
|  | It would get divided by 100.
|  | It would get divided by 10.
V  | It would get multiplied by 100.

### Question 8

I have an outcome, $Y$, and a predictor, $X$ and fit a linear regression model with $Y=\beta_0+\beta_1X+\epsilon$ϵ to obtain $\beta_0$ and $\beta_1$. What would be the consequence to the subsequent slope and intercept if I were to refit the model with a new regressor, $X+c$ for some constant, $c$ ?

Answer  | Options
------- | -------
|  | The new slope would be $c\hat{\beta_1}$
|  | The new intercept would be $\hat{\beta_0}+c\hat{\beta_1}$
|  | The new slope would be $\hat{\beta_1}+c$
V  | The new intercept would be $\hat{\beta_0}-c\hat{\beta_1}$

### Question 9

Refer back to the `mtcars` data set with mpg as an outcome and weight (wt) as the predictor. About what is the ratio of the the sum of the squared errors, $\sum_{i=1}^{n} (Y_i-\hat{Y_i})^2$ when comparing a model with just an intercept (denominator) to the model with the intercept and slope (numerator)?

```{r}
squaredResiduals <- fit$residuals ^ 2
fitCompared <- lm(mpg ~ 1, mtcars)
squaredComparedResiduals <- fitCompared$residuals ^ 2
sum(squaredResiduals)/sum(squaredComparedResiduals)
```

Answer  | Options
------- | -------
V  | 0.25
|  | 0.75
|  | 0.50
|  | 4.00

### Question 10

Do the residuals always have to sum to 0 in linear regression?

Answer  | Options
------- | -------
|  | The residuals never sum to zero.
|  | If an intercept is included, the residuals most likely won't sum to zero.
|  | The residuals must always sum to zero.
V  | If an intercept is included, then they will sum to 0.